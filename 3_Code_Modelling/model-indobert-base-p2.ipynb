{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca857752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Lenovo\\.conda\\envs\\stable_diff_model\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EvalPrediction,\n",
    "    pipeline,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support, matthews_corrcoef, f1_score\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29bab685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment metadata saved to model/indobert-base-p2\n"
     ]
    }
   ],
   "source": [
    "DATA_CSV = \"result/label/df_all_labeled_clean.csv\"  \n",
    "OUT_DIR = \"model/indobert-base-p2\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "exp_meta = {\n",
    "    \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"seed\": SEED,\n",
    "    \"data_csv\": DATA_CSV,\n",
    "    \"model_name\": \"indobenchmark/indobert-base-p2\",\n",
    "    \"notes\": \"HF datasets workflow, sliding-window inference\"\n",
    "}\n",
    "with open(Path(OUT_DIR) / \"exp_meta.json\", \"w\") as f:\n",
    "    json.dump(exp_meta, f, indent=2)\n",
    "print(\"Experiment metadata saved to\", OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49378b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 8992\n",
      "{0: 4443, 1: 3221, 2: 1328}\n"
     ]
    }
   ],
   "source": [
    "# Load CSV into HF Dataset and define label map\n",
    "df = pd.read_csv(DATA_CSV)\n",
    "label_map = {\"Neutral\": 0, \"Inflation\": 1, \"Deflation\": 2}\n",
    "df[\"label_id\"] = df[\"label\"].map(label_map)\n",
    "\n",
    "text_col = \"clean_text\"\n",
    "label_col = \"label_id\"\n",
    "\n",
    "print(\"Total rows:\", len(df))\n",
    "print(df[label_col].value_counts().to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08c796b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8992 entries, 0 to 8991\n",
      "Data columns (total 14 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   url           8992 non-null   object\n",
      " 1   domain        8992 non-null   object\n",
      " 2   title         8992 non-null   object\n",
      " 3   date          8992 non-null   object\n",
      " 4   clean_text    8992 non-null   object\n",
      " 5   label         8992 non-null   object\n",
      " 6   label_reason  8992 non-null   object\n",
      " 7   source        8992 non-null   object\n",
      " 8   year_month    8992 non-null   object\n",
      " 9   year          8992 non-null   int64 \n",
      " 10  month         8992 non-null   int64 \n",
      " 11  tokens        8992 non-null   int64 \n",
      " 12  text_len      8992 non-null   int64 \n",
      " 13  label_id      8992 non-null   int64 \n",
      "dtypes: int64(5), object(9)\n",
      "memory usage: 983.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feeebc1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "Neutral      4443\n",
       "Inflation    3221\n",
       "Deflation    1328\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c564326",
   "metadata": {},
   "source": [
    "# Stratified train / val / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd0f3aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sizes\n",
      "train, val, test:\n",
      " 6294 1349 1349\n"
     ]
    }
   ],
   "source": [
    "train_idx, temp_idx = train_test_split(\n",
    "    df.index.tolist(), test_size=0.30, random_state=SEED, stratify=df[label_col]\n",
    ")\n",
    "val_idx, test_idx = train_test_split(\n",
    "    temp_idx, test_size=0.50, random_state=SEED, stratify=df.loc[temp_idx, label_col]\n",
    ")\n",
    "\n",
    "train_df = df.loc[train_idx].reset_index(drop=True)\n",
    "val_df = df.loc[val_idx].reset_index(drop=True)\n",
    "test_df = df.loc[test_idx].reset_index(drop=True)\n",
    "\n",
    "print(\"Sizes\\ntrain, val, test:\\n\", len(train_df), len(val_df), len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94df07a9",
   "metadata": {},
   "source": [
    "# Tokenizer and tokenization (batched, remove original columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f5a4f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['clean_text', 'labels'],\n",
      "        num_rows: 6294\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['clean_text', 'labels'],\n",
      "        num_rows: 1349\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['clean_text', 'labels'],\n",
      "        num_rows: 1349\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "hf_dset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train_df[[text_col, label_col]]),\n",
    "    \"validation\": Dataset.from_pandas(val_df[[text_col, label_col]]),\n",
    "    \"test\": Dataset.from_pandas(test_df[[text_col, label_col]])\n",
    "})\n",
    "hf_dset = hf_dset.rename_column(\"label_id\", \"labels\")\n",
    "print(hf_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7558def0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\.conda\\envs\\stable_diff_model\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50b79cb5d5de481d83807ba68c7a0b61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6294 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e91742a368a149d7b11b62a0f498b064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b1fe0a7b6ab4d6aa10590c71655a5e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': tensor(2), 'input_ids': tensor([    2,  1117,   300, 12143,  9499,  2193, 18355,  1835, 30469,   502,\n",
      "        30468,  6114,  2569, 30464,  6924, 30363, 30465,  4330,  2876,  5810,\n",
      "        30470,   485,  7081,  9499,  2193, 18355,   339,   262,  8820,  1835,\n",
      "          502, 30468,  4622,  2569, 30464,    31, 30358, 30364, 30465, 30468,\n",
      "           41,   339,  6075,  1835,   111, 30468, 10036,  2569, 30464,  1951,\n",
      "        30371, 30465, 30470, 30458,  1218,  7648, 12265,   405,   126,  1736,\n",
      "         4393,  2193, 18355, 30468,  9499,  2193, 18355,  5429,  1835, 30469,\n",
      "          502, 30468,  6114,  2569, 30464,  6924, 30363, 30465, 30468,   216,\n",
      "         2029,    98,   823,  1131, 30468, 30458,   661,  1179,  4795,  2242,\n",
      "         1117,   300, 30468,   490,   847,  4844,  5680, 30359,  6359,   112,\n",
      "         8472,   460,    57, 30468,   678, 30468,  3253, 30464,  2585, 30471,\n",
      "          606, 30465, 30470, 18583,  3278,    16,  1256,  2876,  5810,   126,\n",
      "         3098,  2490,   644,   245,  1718,    98,  1718,    98, 10326,  5374,\n",
      "         1687,  1835, 30469,   502, 30468,  6105,  2569, 30464,  6924, 30363,\n",
      "        30465, 30468, 10063,  1860, 30468,  6289,    41,  3625,  7407,  2225,\n",
      "         1430, 30469,  1430,  1835, 30469,   502, 30468,  5910,  2569, 30464,\n",
      "         6924, 30363, 30465, 30468, 10063, 21754, 30468,  3637,  4448,    41,\n",
      "         2924,  9664,  1430, 30469,  1430,  1835, 30469,   502, 30468,  4857,\n",
      "         2569, 30464,  6924, 30363, 30465, 30468,   501,  2106,  5259,  1835,\n",
      "        30469,   502, 30468,  6114,  2569, 30464,  6924, 30363, 30465, 30470,\n",
      "            3]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer and tokenization (batched, remove original columns)\n",
    "MODEL_NAME = \"indobenchmark/indobert-base-p2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "max_length = 512  # model limit for base bert\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(batch[text_col], truncation=True, max_length=max_length)\n",
    "\n",
    "hf_dset = hf_dset.map(tokenize_fn, batched=True, remove_columns=[text_col])\n",
    "hf_dset.set_format(type=\"torch\")\n",
    "print(hf_dset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5230b1f5",
   "metadata": {},
   "source": [
    "# Data Collector (dynamic padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f76baa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40c657c",
   "metadata": {},
   "source": [
    "# Compute Class Weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108f1af3",
   "metadata": {},
   "source": [
    "To handle the class imbalance without causing data leakage, we calculate weights using **only the training set distribution** (excluding validation and test sets).\n",
    "\n",
    "The formula used is:\n",
    "\n",
    "$$\n",
    "Weight = \\frac{\\text{Total Training Samples}}{\\text{Number of Classes} \\times \\text{Class Frequency}}\n",
    "$$\n",
    "\n",
    "#### Applied to Training Data:\n",
    "* **Total Training Samples:** $6,294$ ($3110 + 2255 + 929$)\n",
    "* **Number of Classes:** $3$\n",
    "\n",
    "**1. Neutral (Majority Class)**\n",
    "$$\n",
    "W = \\frac{6294}{3 \\times 3110} = \\mathbf{0.67}\n",
    "$$\n",
    "\n",
    "**2. Inflation**\n",
    "$$\n",
    "W = \\frac{6294}{3 \\times 2255} = \\mathbf{0.93}\n",
    "$$\n",
    "\n",
    "**3. Deflation (Minority Class)**\n",
    "$$\n",
    "W = \\frac{6294}{3 \\times 929} = \\mathbf{2.26}\n",
    "$$\n",
    "*(Weight > 1: The model pays 2.26x more attention to this class)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b9a1ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: Counter({0: 3110, 1: 2255, 2: 929})\n",
      "Class weights:  [0.6745980707395498, 0.9303769401330377, 2.2583423035522068]\n"
     ]
    }
   ],
   "source": [
    "train_labels = train_df[label_col].to_list()\n",
    "counts = Counter(train_labels)\n",
    "total=sum(counts.values())\n",
    "num_classes = len(label_map)\n",
    "class_weights_list = [total / (num_classes*counts.get(i,1)) for i in range (num_classes)]\n",
    "class_weights = torch.tensor(class_weights_list, dtype = torch.float)\n",
    "print(\"Class counts:\", counts)\n",
    "print(\"Class weights: \", class_weights_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d03918",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b4bf5c",
   "metadata": {},
   "source": [
    "# Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65b5b70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred: EvalPrediction):\n",
    "    logits = pred.predictions\n",
    "    y_true = pred.label_ids\n",
    "    y_pred = np.argmax(logits, axis=-1)\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=\"macro\", zero_division=0\n",
    "    )\n",
    "\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "    report = {\n",
    "        \"accuracy\": (y_true == y_pred).mean(),\n",
    "        \"f1_macro\": f1,\n",
    "        \"precision_macro\": precision,\n",
    "        \"recall_macro\": recall,\n",
    "        \"mcc\": mcc\n",
    "    }\n",
    "    return report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e64bc7d",
   "metadata": {},
   "source": [
    "# Pre-training Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4646edc9",
   "metadata": {},
   "source": [
    "This code is for sanitiy check to ensure that:\n",
    "- The model is successfully loaded into memory (GPU/CPU).\n",
    "- The test data was successfully entered into the model without error (tensor dimensions matched).\n",
    "- The evaluation function (compute_metrics) runs without bugs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ca50a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\.conda\\envs\\stable_diff_model\\Lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "baseline_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_classes,\n",
    "    local_files_only=True\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f351da73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Performance (PRE-TRAIN):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b86552be8404db29c3433fa7df980d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/169 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "baseline_trainer = Trainer(\n",
    "    model=baseline_model,\n",
    "    eval_dataset=hf_dset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "# 3. Evaluate without training\n",
    "print(\"Baseline Performance (PRE-TRAIN):\")\n",
    "baseline_metrics = baseline_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7c63dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0758129358291626, 'eval_model_preparation_time': 0.0, 'eval_accuracy': 0.3535952557449963, 'eval_f1_macro': 0.17754604695962142, 'eval_precision_macro': 0.20161290322580647, 'eval_recall_macro': 0.32843102258394613, 'eval_mcc': -0.050910245767553146, 'eval_runtime': 290.6467, 'eval_samples_per_second': 4.641, 'eval_steps_per_second': 0.581}\n"
     ]
    }
   ],
   "source": [
    "print(baseline_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8da6858d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a64d405022a4d35a9637b7392d23299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/169 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Neutral       0.25      0.01      0.01       667\n",
      "   Inflation       0.35      0.98      0.52       483\n",
      "   Deflation       0.00      0.00      0.00       199\n",
      "\n",
      "    accuracy                           0.35      1349\n",
      "   macro avg       0.20      0.33      0.18      1349\n",
      "weighted avg       0.25      0.35      0.19      1349\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\.conda\\envs\\stable_diff_model\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\Lenovo\\.conda\\envs\\stable_diff_model\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\Lenovo\\.conda\\envs\\stable_diff_model\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "base_output = baseline_trainer.predict(hf_dset[\"test\"])\n",
    "base_y_pred = np.argmax(base_output.predictions, axis=1)\n",
    "base_y_true = base_output.label_ids\n",
    "\n",
    "target_names = [\"Neutral\", \"Inflation\", \"Deflation\"]\n",
    "print(\"\\nBaseline Classification Report\")\n",
    "print(classification_report(base_y_true, base_y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45e480a",
   "metadata": {},
   "source": [
    "It also shows the data integrity. The evaluation results on the log show an accuracy of 0.35 (35%). Since there are 3 classes (Neutral, Inflation, Deflation), the probability of a random guess is 1/3â‰ˆ33%. A yield of 35% is very close to 33%.\n",
    "\n",
    "Also with the f1-macro of 0.18 indicating the model needed fine tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3f87bf",
   "metadata": {},
   "source": [
    "# Fine Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38e1cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\.conda\\envs\\stable_diff_model\\Lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(50000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    num_labels = num_classes,\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207a8fc5",
   "metadata": {},
   "source": [
    "# Create Class WeightedTrainer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f35fa6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**{k: v for k, v in inputs.items() if k != \"labels\"})\n",
    "        logits = outputs.logits\n",
    "\n",
    "        weight = class_weights.to(model.device)\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=weight, label_smoothing=0.1)\n",
    "        loss = loss_fct(logits.view(-1, model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9872b92",
   "metadata": {},
   "source": [
    "# Training Arguments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e7d557b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\.conda\\envs\\stable_diff_model\\Lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Training arguments and trainer creation\n",
    "output_dir = Path(OUT_DIR) / \"model\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(output_dir),\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    greater_is_better=True,\n",
    "\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "\n",
    "    optim=\"adamw_torch\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_grad_norm=1.0,\n",
    "    group_by_length=True,\n",
    "\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    logging_steps=50,\n",
    "    seed=SEED,\n",
    "    save_total_limit=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc21277",
   "metadata": {},
   "source": [
    "# Weighted Trainer Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffc9a94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=hf_dset[\"train\"],\n",
    "    eval_dataset=hf_dset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e59f42",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75d71919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcb5171ff0e54d46aa01048ecc8bc712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1960 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1612, 'grad_norm': 5.37812614440918, 'learning_rate': 5.1020408163265315e-06, 'epoch': 0.25}\n",
      "{'loss': 1.0713, 'grad_norm': 4.552521228790283, 'learning_rate': 1.0204081632653063e-05, 'epoch': 0.51}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d344ce120854fe1b5737b7f4938dd7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0142443180084229, 'eval_accuracy': 0.4951816160118606, 'eval_f1_macro': 0.4644816661407761, 'eval_precision_macro': 0.5865261784021548, 'eval_recall_macro': 0.5478007510616206, 'eval_mcc': 0.33778971082721265, 'eval_runtime': 30.2463, 'eval_samples_per_second': 44.601, 'eval_steps_per_second': 2.81, 'epoch': 0.51}\n",
      "{'loss': 1.0215, 'grad_norm': 4.11823034286499, 'learning_rate': 1.530612244897959e-05, 'epoch': 0.76}\n",
      "{'loss': 0.9487, 'grad_norm': 4.099294662475586, 'learning_rate': 1.9999746258949146e-05, 'epoch': 1.02}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de4263c09b554fe6b46d474e7ee52576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9053704142570496, 'eval_accuracy': 0.7071905114899926, 'eval_f1_macro': 0.6633026893245205, 'eval_precision_macro': 0.6637798227965924, 'eval_recall_macro': 0.6647218336348771, 'eval_mcc': 0.52127234471594, 'eval_runtime': 35.3637, 'eval_samples_per_second': 38.147, 'eval_steps_per_second': 2.404, 'epoch': 1.02}\n",
      "{'loss': 0.8719, 'grad_norm': 4.808755397796631, 'learning_rate': 1.9953791129491985e-05, 'epoch': 1.27}\n",
      "{'loss': 0.8861, 'grad_norm': 5.3804497718811035, 'learning_rate': 1.9828960137631927e-05, 'epoch': 1.52}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "320bb456585d4d53bcf5ead0e9463591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8623097538948059, 'eval_accuracy': 0.7316530763528539, 'eval_f1_macro': 0.6998681823446246, 'eval_precision_macro': 0.6935704543936891, 'eval_recall_macro': 0.710686618295314, 'eval_mcc': 0.5679121162475496, 'eval_runtime': 34.1804, 'eval_samples_per_second': 39.467, 'eval_steps_per_second': 2.487, 'epoch': 1.52}\n",
      "{'loss': 0.8608, 'grad_norm': 6.842891693115234, 'learning_rate': 1.962624246950012e-05, 'epoch': 1.78}\n",
      "{'loss': 0.8461, 'grad_norm': 7.770502090454102, 'learning_rate': 1.934724450106831e-05, 'epoch': 2.03}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87b103e69a00438baa0d3100988a66cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8772342801094055, 'eval_accuracy': 0.7449962935507783, 'eval_f1_macro': 0.7085693888830938, 'eval_precision_macro': 0.7106444601742746, 'eval_recall_macro': 0.7087731520340216, 'eval_mcc': 0.5787986937413553, 'eval_runtime': 34.5468, 'eval_samples_per_second': 39.049, 'eval_steps_per_second': 2.46, 'epoch': 2.03}\n",
      "{'loss': 0.7911, 'grad_norm': 8.671582221984863, 'learning_rate': 1.8994177068899414e-05, 'epoch': 2.29}\n",
      "{'loss': 0.743, 'grad_norm': 7.371335029602051, 'learning_rate': 1.8569837951029597e-05, 'epoch': 2.54}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80799038d62146ada9a38c3a1d264ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8445148468017578, 'eval_accuracy': 0.7501853224610823, 'eval_f1_macro': 0.7226010401432541, 'eval_precision_macro': 0.7186973328845156, 'eval_recall_macro': 0.7332960289482028, 'eval_mcc': 0.592805001084145, 'eval_runtime': 30.9393, 'eval_samples_per_second': 43.602, 'eval_steps_per_second': 2.747, 'epoch': 2.54}\n",
      "{'loss': 0.7836, 'grad_norm': 3.0469863414764404, 'learning_rate': 1.8077589696806925e-05, 'epoch': 2.8}\n",
      "{'loss': 0.7227, 'grad_norm': 5.77237606048584, 'learning_rate': 1.752133298136744e-05, 'epoch': 3.05}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8146c823fb6a413f880f066cb6f85595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.831396222114563, 'eval_accuracy': 0.7160859896219421, 'eval_f1_macro': 0.6883551444681651, 'eval_precision_macro': 0.6787691289936082, 'eval_recall_macro': 0.7147673760717238, 'eval_mcc': 0.5550979527192643, 'eval_runtime': 33.0706, 'eval_samples_per_second': 40.791, 'eval_steps_per_second': 2.57, 'epoch': 3.05}\n",
      "{'loss': 0.6675, 'grad_norm': 9.59798812866211, 'learning_rate': 1.6905475695893193e-05, 'epoch': 3.3}\n",
      "{'loss': 0.6782, 'grad_norm': 5.1205525398254395, 'learning_rate': 1.6234898018587336e-05, 'epoch': 3.56}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc972c33003549b08a7c827dea1016fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8862666487693787, 'eval_accuracy': 0.748702742772424, 'eval_f1_macro': 0.7161389079418589, 'eval_precision_macro': 0.7246721047290317, 'eval_recall_macro': 0.712688216166477, 'eval_mcc': 0.5836752081230322, 'eval_runtime': 28.7434, 'eval_samples_per_second': 46.932, 'eval_steps_per_second': 2.957, 'epoch': 3.56}\n",
      "{'loss': 0.6543, 'grad_norm': 6.635543346405029, 'learning_rate': 1.551491374315094e-05, 'epoch': 3.81}\n",
      "{'loss': 0.6615, 'grad_norm': 7.628347873687744, 'learning_rate': 1.475122817120253e-05, 'epoch': 4.07}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8561a1aa5c944ce92e41c1c4c22066b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8591805100440979, 'eval_accuracy': 0.7316530763528539, 'eval_f1_macro': 0.7050706533966898, 'eval_precision_macro': 0.6950774039481179, 'eval_recall_macro': 0.7296961557831123, 'eval_mcc': 0.5783393334461833, 'eval_runtime': 27.6076, 'eval_samples_per_second': 48.863, 'eval_steps_per_second': 3.079, 'epoch': 4.07}\n",
      "{'train_runtime': 2122.1313, 'train_samples_per_second': 29.659, 'train_steps_per_second': 0.924, 'train_loss': 0.8356060314178467, 'epoch': 4.07}\n"
     ]
    }
   ],
   "source": [
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ec9e43",
   "metadata": {},
   "source": [
    "# Save Model & Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58ad84d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "with open(Path(OUT_DIR) / \"train_result.json\", \"w\") as f:\n",
    "    json.dump(train_result.metrics, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556738be",
   "metadata": {},
   "source": [
    "# Thresholding Minority Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca93d1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc2ec4fd45fc400cb3e4dfc22152d4c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Threshold: 0.50 (Val F1: 0.6306)\n"
     ]
    }
   ],
   "source": [
    "val_output = trainer.predict(hf_dset[\"validation\"])\n",
    "val_probs = torch.nn.functional.softmax(torch.tensor(val_output.predictions), dim=1).numpy()\n",
    "val_labels = val_output.label_ids\n",
    "\n",
    "TARGET_CLASS_ID = 2  # Deflation\n",
    "best_threshold = 0.5\n",
    "best_f1 = 0.0\n",
    "\n",
    "# Search for best threshold\n",
    "for thr in np.arange(0.1, 0.95, 0.05):\n",
    "    preds = []\n",
    "    for i in range(len(val_probs)):\n",
    "        if val_probs[i, TARGET_CLASS_ID] >= thr:\n",
    "            preds.append(TARGET_CLASS_ID)\n",
    "        else:\n",
    "            preds.append(np.argmax(val_probs[i]))\n",
    "            \n",
    "    # Calculate F1 for the target class only\n",
    "    # We use the validation set to pick the winner\n",
    "    current_f1 = f1_score(val_labels, preds, labels=[TARGET_CLASS_ID], average='micro')\n",
    "    \n",
    "    if current_f1 > best_f1:\n",
    "        best_f1 = current_f1\n",
    "        best_threshold = thr\n",
    "\n",
    "print(f\"Optimal Threshold: {best_threshold:.2f} (Val F1: {best_f1:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3fd163c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_data = {\n",
    "    \"target_class\": \"Deflation\",\n",
    "    \"target_id\": TARGET_CLASS_ID,\n",
    "    \"threshold\": float(best_threshold)\n",
    "}\n",
    "\n",
    "with open(Path(OUT_DIR) / \"threshold_config.json\", \"w\") as f:\n",
    "    json.dump(config_data, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc72ee6",
   "metadata": {},
   "source": [
    "# Comparison Threshold vs No Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d435898d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43097687b51645919c781fef8f40edf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get Test Predictions\n",
    "test_output = trainer.predict(hf_dset[\"test\"])\n",
    "test_probs = torch.nn.functional.softmax(torch.tensor(test_output.predictions), dim=1).numpy()\n",
    "test_labels = test_output.label_ids\n",
    "\n",
    "# SCENARIO A: Standard Prediction, what the model does by default\n",
    "test_preds_standard = np.argmax(test_probs, axis=1)\n",
    "\n",
    "# SCENARIO B: Thresholded Prediction with the threshold\n",
    "test_preds_tuned = []\n",
    "for i in range(len(test_probs)):\n",
    "    if test_probs[i, TARGET_CLASS_ID] >= best_threshold:\n",
    "        test_preds_tuned.append(TARGET_CLASS_ID)\n",
    "    else:\n",
    "        test_preds_tuned.append(np.argmax(test_probs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a00b505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCENARIO A: Standard Model (No Thresholding)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Neutral       0.78      0.77      0.78       667\n",
      "   Inflation       0.78      0.69      0.73       483\n",
      "   Deflation       0.52      0.68      0.59       199\n",
      "\n",
      "    accuracy                           0.73      1349\n",
      "   macro avg       0.69      0.72      0.70      1349\n",
      "weighted avg       0.74      0.73      0.73      1349\n",
      "\n",
      "[[515  71  81]\n",
      " [104 335  44]\n",
      " [ 39  24 136]]\n",
      "\n",
      "\n",
      "SCENARIO B: Optimized Model (Threshold > 0.50)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Neutral       0.78      0.77      0.78       667\n",
      "   Inflation       0.78      0.69      0.73       483\n",
      "   Deflation       0.52      0.68      0.59       199\n",
      "\n",
      "    accuracy                           0.73      1349\n",
      "   macro avg       0.69      0.72      0.70      1349\n",
      "weighted avg       0.74      0.73      0.73      1349\n",
      "\n",
      "[[515  71  81]\n",
      " [104 335  44]\n",
      " [ 39  24 136]]\n"
     ]
    }
   ],
   "source": [
    "target_names = [\"Neutral\", \"Inflation\", \"Deflation\"] \n",
    "\n",
    "print(\"SCENARIO A: Standard Model (No Thresholding)\")\n",
    "print(classification_report(test_labels, test_preds_standard, target_names=target_names))\n",
    "print(confusion_matrix(test_labels, test_preds_standard))\n",
    "print(f\"\\n\\nSCENARIO B: Optimized Model (Threshold > {best_threshold:.2f})\")\n",
    "print(classification_report(test_labels, test_preds_tuned, target_names=target_names))\n",
    "print(confusion_matrix(test_labels, test_preds_tuned))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd4155d",
   "metadata": {},
   "source": [
    "No imporvement rather use the Standard Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a630cfd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics saved to model/indobert-base-p2\n"
     ]
    }
   ],
   "source": [
    "# save metrics\n",
    "metrics = {\n",
    "    \"test_classification_report\": classification_report(test_labels, test_preds_standard, target_names=[\"Neutral\", \"Inflation\", \"Deflation\"], \n",
    "    output_dict=True, zero_division=0),\n",
    "    \"test_confusion_matrix\": confusion_matrix(test_labels, test_preds_standard).tolist(),\n",
    "    \"thresholds\": best_threshold\n",
    "}\n",
    "with open(Path(OUT_DIR) / \"test_metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "print(\"Test metrics saved to\", OUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2480dcef",
   "metadata": {},
   "source": [
    "# Sliding Window Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ac97bf",
   "metadata": {},
   "source": [
    "Since IndoBERT has a hard limit of 512 tokens, it crashes if you feed it a long news report (1000-word analysis of the Indonesian economy). This function creates a work-around called Sliding Window Inference.\n",
    "\n",
    "\n",
    "How it works: overlapping slightly so it doesn't miss the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a390d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sliding_window_clean(text, model, tokenizer, device, chunk_size=512, stride=256):\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize the whole text\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "    \n",
    "    # IF Short Text (Fits in one block)\n",
    "    if len(tokens) <= chunk_size:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=chunk_size)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            # Standard Softmax (Temperature=1.0 is default behavior)\n",
    "            probs = F.softmax(outputs.logits, dim=1).cpu().numpy()[0]\n",
    "        \n",
    "        pred_label = np.argmax(probs)\n",
    "        return pred_label, probs\n",
    "\n",
    "    # IF Long Text (Sliding Window)\n",
    "    logits_list = []\n",
    "    \n",
    "    # Loop through tokens with overlap (stride)\n",
    "    for start in range(0, len(tokens), stride):\n",
    "        # Slice the tokens\n",
    "        window = tokens[start : start + chunk_size]\n",
    "        \n",
    "        # Stop if we have a tiny leftover chunk (optional safety)\n",
    "        if len(window) < 10: \n",
    "            break\n",
    "            \n",
    "        # Decode back to text and Re-encode\n",
    "        # This ensures every chunk gets its own [CLS] and [SEP] tokens\n",
    "        window_text = tokenizer.decode(window, skip_special_tokens=True)\n",
    "        inputs = tokenizer(window_text, return_tensors=\"pt\", truncation=True, max_length=chunk_size)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            out = model(**inputs)\n",
    "            logits_list.append(out.logits.cpu().numpy())\n",
    "            \n",
    "        # Stop if we reached the end\n",
    "        if start + chunk_size >= len(tokens):\n",
    "            break\n",
    "    \n",
    "    # 1. Stack all logits (shape: [num_chunks, num_classes])\n",
    "    all_logits = np.vstack(logits_list)\n",
    "    \n",
    "    # 2. Average the logits (Consensus mechanism)\n",
    "    avg_logits = np.mean(all_logits, axis=0)\n",
    "    \n",
    "    # 3. Convert Average Logits to Probability\n",
    "    probs = F.softmax(torch.tensor(avg_logits), dim=0).numpy()\n",
    "    \n",
    "    # 4. Pick Winner\n",
    "    pred_label = np.argmax(probs)\n",
    "    \n",
    "    return pred_label, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707eab8e",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344215af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Inflation\n",
      "Confidence: 0.5718\n",
      "Full Probs: [0.2970798  0.571798   0.13112219]\n"
     ]
    }
   ],
   "source": [
    "text_example ='Dewan Pengupahan Jawa Timur dari Unsur Buruh mengungkap keinginan para buruh di Jatim agar upah minimum kabupaten/kota (UMK) naik 8-10%. Inflasi menjadi salah satu alasan buruh agar upah naik 8-10%.Harapannya para buruh untuk UMK 38 kabupaten/kota di Jatim naik 8 sampai 10%, kata Ketua Dewan Pengupahan Jatim Unsur Buruh, Ahmad Fauzi di Surabaya, Rabu (12/11/2025). Fauzi menyebut Dewan Pengupahan Jatim akan segera membahas UMK dan Upah Minimum Provinsi (UMP) di mana UMP di-deadline akan diumumkan pada awal Desember, sedangkan UMK pada pertengahan Desember.Rencananya penetapan UMP dijadwalkan pada 8 Desember 2025, sementara UMK akan ditetapkan pada 15 Desember 2025Lebih lanjut kata Fauzi, para buruh ingin kenaikan upah di angka 8-10% sebab biaya hidup semakin meningkat diiringi dengan adanya inflasi.'\n",
    "\n",
    "pred_id, probabilities = predict_sliding_window_clean(\n",
    "    text_example, \n",
    "    model, \n",
    "    tokenizer, \n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "labels = [\"Neutral\", \"Inflation\", \"Deflation\"]\n",
    "print(f\"Prediction: {labels[pred_id]}\")\n",
    "print(f\"Confidence: {probabilities[pred_id]:.4f}\")\n",
    "print(f\"Full Probs: {probabilities}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1de3702",
   "metadata": {},
   "source": [
    "# Jawaban AI Gemini 3.0 terhadap text tersebut\n",
    "\n",
    "ECONOMIC ASSESSMENT\n",
    "\n",
    "\n",
    "PROBABILITIES\n",
    "* Inflationary: 75%\n",
    "* Neutral (Stable): 20%\n",
    "* Deflationary: 5%\n",
    "\n",
    "\n",
    "REASONING\n",
    "\n",
    "\n",
    "Wage-Push Pressure (Inflation Driver)\n",
    "The core signal in this text is the demand for an 8-10% increase in the minimum wage (UMK). In economic theory, a significant rise in the wage floor often triggers \"cost-push inflation.\" Businesses faced with higher labor costs typically raise prices on goods and services to maintain their profit margins, directly contributing to general price increases.\n",
    "\n",
    "\n",
    "Erosion of Purchasing Power (Inflation Driver)\n",
    "The text explicitly cites \"biaya hidup semakin meningkat\" (rising cost of living) as the primary justification for the wage demand. This confirms that inflation is currently present and felt by the consumer base. The labor union is reacting to existing price instability, attempting to restore real income levels that have been eroded by rising prices.\n",
    "\n",
    "\n",
    "Inflation Expectations (Inflation Driver)\n",
    "The specific target of 8-10% suggests that economic actors (workers) expect inflation to continue or accelerate. When a large segment of the workforce anticipates higher prices and negotiates wages based on that expectation, it can create a self-fulfilling cycle known as a wage-price spiral, where higher wages fuel further demand and price hikes.\n",
    "\n",
    "\n",
    "Policy Uncertainty (Neutral Driver)\n",
    "It is important to note that the 8-10% figure is currently a \"keinginan\" (desire) or proposal from the labor union, not the finalized government decree. If the final decision (scheduled for mid-December) settles on a lower percentage closer to the national GDP growth rate, the inflationary impact would be contained, resulting in a neutral economic outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23298ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Neutral\n",
      "Confidence: 0.6048\n",
      "Full Probs: [0.604755   0.21125492 0.1839901 ]\n"
     ]
    }
   ],
   "source": [
    "long_text = \"KBRN, Tarakan: Perekonomian Indonesia masih tumbuh ditengah kondisi ekonomi global yang kurang baik. Meski melambat, ekonomi Indoensia tumbuh 5,4â„…. Hal itu disampaikan Kepala Perwakilan Bank Indonesia Kaltara, Hasiando Ginsar Manik saat menjadi narasumber pada Benuanta Investment and Economic Summit di Kayan Multifunction Hall Hotel Tarakan Plaza, Jumat (21/2011/2025). 'Ekonomi kita di triwulan triwulan III tumbuh 5,4% melambat dibandingkan triwulan II tahun, 2025 yang mana ditopang oleh kinerja ekspor dan konsumsi pemerintah yang meningkat di triwulan III,' ungkap Hasiando. Beberapa sumber-sumber pertumbuhan ekonomi Indonesia di antaranya di Kalimantan yang mampu menyumbangkan 8,02%. Sedangkan porsi ekonomi utama masih berasal dari Jawa dan Sumatera. Menurutnya, tantangan ekonomi nasional saat ini adalah bagaimana menjaga pertumbuhan ekonomi Indonesia. Di sisi lain stabilitas inflasi sebenarnya terkendali. Akan tetapi menurut Hasiando, ada satu tantangan untuk investasi. Yaitu inflasi volitale food. Di mana ada beberapa daerah yang inflasi volitale foodnya di atas 5%, terutama wilayah Sumatera. Sementara wilayah Kalimantan stabilitas harganya relatif terkendali. 'Itu menjadi tantangan tersendiri bagi kita semua,' tutur Hasiando. Menurutnya, berbagai upaya telah dilakukan pemerintah pusat melalui kementerian terkait maupun pemerintah daerah agar pertumbuhan ekonomi Indonesia pada 2025 bisa bisa meningkat. Terlebih target Presiden Prabowo Subianto untuk perekonomian Indonesia tumbuh 8% pada 2028. Bank Indonesia sendiri melalui Dewan Gubernur telah memutuskan BI Ret tetap 4,65â„…. Tugas BI sendiri ada dua. Yaitu menjaga stabilitas, baiknya nilai tukar, harga dan sistem keuangan. Sedangkan kedua adalah upaya mendukung pertumbuhan ekonomi. Adapun kondisi konomi global, menurut Hasiando, sebenarnya tidak terlalu baik. Di mana pertumbuhan ekonomi di dunia di 2025 diperkirakan turun dari 3,3% menjadi 3,1%. Sebagian besar negara merevisi melambat pertumbuhannya. Kondisi ini tentu ada sebabnya. Mulai dari kondisi Amerika Serikat di mana banyak fasilitas pemerintahan yang tidak bekerja optimal. Ditambah lagi perang dagang antara Amerika Serikat dan Cina menyebabkan potensi demand, termasuk Indonesia, mengalami perlambatan. Kepastian ekonomi global juga tercermin dari beberapa indeks ketidakpastian dan polabilitas global. Menurut Hasiando, memang ada tren menurun. Akan tetapi jika melihat history dari tahun 2021 sampai sekarang masih di level atas. Benuanta Investment and Economic Summit merupakan acara yang digelar Kantor Perwakilan Bank Indonesia (KPwBI) Provinsi Kalimantan Utara (Kaltara). Acara yang dirangkai dengan diskusi panel ini dibuka Wakil Gubernur Kaltara, Ingkong Ala, dengan menghadirkan berbagai narasumber. Di antaranya Kepala Perwakilan Bank Indonesia Kaltara, Hasiando Ginsar Manik, Staf Ahli Menteri Keuangan Bidang akepatuhan Pajak, Yon Arsal, Procipal Adviser, Revenue (Police and Administration) At Proses, Rubino Sugana dan Direktur Politeknik Bisnis Kaltara, Dr. Ana Sriekaningsih, S.E., S.Th., M.M. Acara ini mengusung tema 'Epicentrum Pertumbuhan Baru Menakar Peran Hilirisasi, Industrialisasi dan Konektivitas Global terhadap Perekonomian Daerah'. (Rajab)\"\n",
    "\n",
    "pred_id, probabilities = predict_sliding_window_clean(\n",
    "    long_text, \n",
    "    model, \n",
    "    tokenizer, \n",
    "    device=DEVICE\n",
    ")   \n",
    "\n",
    "print(f\"Prediction: {labels[pred_id]}\")\n",
    "print(f\"Confidence: {probabilities[pred_id]:.4f}\")\n",
    "print(f\"Full Probs: {probabilities}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5891829d",
   "metadata": {},
   "source": [
    "# Jawaban AI Gemini 3.0 terhadap text tersebut\n",
    "\n",
    "ECONOMIC ASSESSMENT\n",
    "\n",
    "\n",
    "PROBABILITIES\n",
    "* Neutral (Stable): 60%\n",
    "* Inflationary: 35%\n",
    "* Deflationary: 5%\n",
    "\n",
    "\n",
    "REASONING\n",
    "\n",
    "\n",
    "Explicit Stability (Neutral Driver)\n",
    "The text explicitly states that inflation stability is controlled. Bank Indonesia decided to hold the BI Rate at 4.65%. Central banks typically hold rates steady when they believe the economy is balanced. They are not raising rates to fight high inflation. They are not lowering rates to fight deflation.\n",
    "\n",
    "\n",
    "Volatile Food Prices (Inflation Driver)\n",
    "The primary risk mentioned is \"volatile food.\" Inflation in this sector exceeds 5% in specific regions like Sumatra. This serves as a strong counter-argument to deflation. Prices for essentials are rising in some areas. This creates localized cost-push inflation even if the national average is stable.\n",
    "\n",
    "\n",
    "Slowing Global Demand (Deflation Driver)\n",
    "The global economy is slowing down. Growth dropped from 3.3% to 3.1%. The text mentions trade wars and reduced demand from China and the US. Lower global demand typically lowers commodity prices. This external factor helps keep domestic inflation from getting too high.\n",
    "\n",
    "\n",
    "Positive GDP Growth (General Health)\n",
    "Indonesia's economy grew by 5.4%. Deflation is usually associated with economic contraction or recession. Since the economy is expanding, general deflation is highly unlikely. The slowdown from Q2 to Q3 suggests cooling, not freezing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417f7e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Deflation\n",
      "Confidence: 0.9744\n",
      "Full Probs: [0.01147786 0.01411801 0.9744042 ]\n"
     ]
    }
   ],
   "source": [
    "text_another_one = 'Kota Pematangsiantar mencatat deflasi sebesar -0,31 persen (mtm) pada Oktober 2025. Penurunan indeks harga ini terutama dipicu turunnya harga sejumlah komoditas pangan, terutama bawang merah dan cabai hijau.Kepala BPS Kota Pematangsiantar, Ratnauli Naibaho melalui Staf Statistik Harga, Wahyu Andamari, menyampaikan bahwa bawang merah menjadi penyumbang deflasi terbesar pada Oktober dengan andil -0,15 persen, disusul cabai hijau -0,09 persen, serta beras -0,07 persen.\"Penurunan harga komoditas hortikultura ini terjadi karena pasokan meningkat di wilayah Sumatera Utara, seiring masuknya masa panen pada bulan Oktober,\" ujar Wahyu kepada Mistar, Kamis (20/11/2025).Ia menjelaskan, deflasi pada bulan tersebut tidak lepas dari membaiknya pasokan bahan pangan, dipengaruhi kondisi cuaca, biaya produksi, dan keseimbangan antara permintaan serta penawaran di pasar.Meski demikian, sejumlah komoditas masih memberikan andil terhadap inflasi, seperti emas perhiasan 0,29 persen, cabai merah 0,04 persen, dan wortel 0,03 persen.Wahyu menambahkan, tekanan inflasi diperkirakan terus menurun pada November 2025. Prediksi curah hujan yang meningkat di Sumatera Utara dinilai akan mendorong hasil panen lebih baik, sehingga harga sejumlah komoditas pangan strategis berpotensi kembali turun.\"Dengan hasil panen yang meningkat, potensi penurunan tekanan inflasi pada periode mendatang semakin besar,\" ujarnya. (hm25)'\n",
    "\n",
    "pred_id, probabilities = predict_sliding_window_clean(\n",
    "    text_another_one, \n",
    "    model, \n",
    "    tokenizer, \n",
    "    device=DEVICE\n",
    ")   \n",
    "\n",
    "print(f\"Prediction: {labels[pred_id]}\")\n",
    "print(f\"Confidence: {probabilities[pred_id]:.4f}\")\n",
    "print(f\"Full Probs: {probabilities}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768f0814",
   "metadata": {},
   "source": [
    "# Jawaban AI Gemini 3.0 terhadap text tersebut\n",
    "\n",
    "ECONOMIC ASSESSMENT\n",
    "\n",
    "\n",
    "PROBABILITIES\n",
    "* Deflationary: 85%\n",
    "* Neutral (Stable): 10%\n",
    "* Inflationary: 5%\n",
    "\n",
    "\n",
    "REASONING\n",
    "\n",
    "\n",
    "Realized Deflationary Data (Deflation Driver)\n",
    "The text provides concrete statistical evidence of deflation, stating explicitly that Kota Pematangsiantar recorded a deflation of -0.31% (month-to-month) in October 2025. Unlike previous texts that relied on forecasts or demands, this is realized economic data confirming that the general price level has already decreased.\n",
    "\n",
    "\n",
    "Supply-Side Surplus (Deflation Driver)\n",
    "The primary mechanism driving this trend is a positive supply shock in the food sector. The text attributes the price drops to increased supply (\"pasokan meningkat\") caused by the harvest season (\"masa panen\"). In economics, when supply exceeds demand due to seasonal factors like a harvest, equilibrium prices naturally fall. Major contributors like shallots and green chilies drove the index down.\n",
    "\n",
    "\n",
    "Forward-Looking Price Pressure (Deflation Driver)\n",
    "The outlook for the immediate future remains deflationary or low-pressure. The BPS official predicts that inflationary pressure will continue to decline in November 2025 due to weather conditions (increased rainfall) favoring agricultural output. This expectation of continued abundance suggests that prices for strategic foods will likely remain low or drop further.\n",
    "\n",
    "\n",
    "Commodity Divergence (Inflation Driver)\n",
    "While the aggregate index is deflationary, specific assets show inflationary resilience. Gold jewelry rose by 0.29%. This indicates that while food costs are dropping (lowering the cost of living), store-of-value assets or non-perishables are still seeing price appreciation. However, the weight of food commodities in the consumer basket is currently overpowering these increases, resulting in net deflation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stable_diff_model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
