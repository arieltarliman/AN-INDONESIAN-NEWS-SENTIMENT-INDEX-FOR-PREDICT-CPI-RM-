{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f158280",
   "metadata": {},
   "source": [
    "# Fidelity Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c33e8b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In this notebook we will do a comparison of 3 trained models the students (mdeberta-v3-base, indolem-indobert-base-uncased, indobert-base-p2) with the teacher Gemini 2.5 Flash.\n",
    "\n",
    "We are not evaluate model performance on a real-world groundf truth. We are evaluation model agreement with Gemini, because Gemini is the teacher of these 3 models. Or other people says a teacher-student alignment or distillation fidelity. We want to measure how much does the model mimic the behavior of Gemini on new, unseen economic text. \n",
    "\n",
    "\n",
    "Yes on the 3 model notebook, we already evaluate the model perfromance by metrics such as Macro F1 and accuracy between 3 of the models. But that is only shows perfromance on the golden dataset labels that has been labelled by Gemini 2.5 Flash. That doesn't tell ghow similar their behaviour is to Gemini on new text. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862b4116",
   "metadata": {},
   "source": [
    "We have already done creatiung a new dataset that Gemini has never labeled on EDA_n_Preprocessing_links.ipynb & EDA_n_Preprocessing_Scraping_Result.ipynb with the help of scraping script scraping.py\n",
    "\n",
    "Then, we let GEmini to label them & now we run all 3 models to produce predictions on the asme data.\n",
    "\n",
    "We compare each student model to gemini using:\n",
    "- accuracy\n",
    "- macro F1\n",
    "- Cohen's kappa\n",
    "- KL divergence of probability vectors \n",
    "- Jensen-Shannon distance\n",
    "- Correlation across class logits\n",
    "\n",
    "Then we analyze where the students disagree with Gemini.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cab08421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, cohen_kappa_score, confusion_matrix\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from scipy.spatial.distance import jensenshannon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacf3de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df_all_labeled.csv')\n",
    "df = df.drop(columns=['label','label_reason'])\n",
    "texts = df[\"clean_text\"].astype(str).tolist()\n",
    "\n",
    "df_gemini = pd.read_csv('df_all_labeled.csv')\n",
    "label_map = {\"Neutral\":0, \"Inflation\":1, \"Deflation\":2}\n",
    "y_true = df_gemini[\"label\"].map(label_map).astype(int).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63a6852c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model+tokenizer\n",
    "def load_model(path):\n",
    "    tok = AutoTokenizer.from_pretrained(path)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(path)\n",
    "    model.eval()\n",
    "    return tok, model\n",
    "\n",
    "# predict in batches with hard max_length=512\n",
    "def predict_batch(texts, tok, model, batch=16):\n",
    "    out_labels = []\n",
    "    out_probs = []\n",
    "    for i in range(0, len(texts), batch):\n",
    "        chunk = texts[i:i+batch]\n",
    "        enc = tok(\n",
    "            chunk,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            logits = model(**enc).logits\n",
    "        probs = torch.softmax(logits, dim=1).numpy()\n",
    "        preds = probs.argmax(axis=1)\n",
    "        out_labels.extend(preds)\n",
    "        out_probs.extend(probs)\n",
    "    return np.array(out_labels), np.array(out_probs)\n",
    "\n",
    "# fidelity evaluation\n",
    "def js_divergence(p, q):\n",
    "    return jensenshannon(p, q)\n",
    "\n",
    "def evaluate(name, preds, probs, y_true):\n",
    "    acc = accuracy_score(y_true, preds)\n",
    "    f1 = f1_score(y_true, preds, average=\"macro\")\n",
    "    kappa = cohen_kappa_score(y_true, preds)\n",
    "    cm = confusion_matrix(y_true, preds)\n",
    "\n",
    "    # one-hot Gemini\n",
    "    oh = np.eye(3)[y_true]\n",
    "    js = np.mean([js_divergence(probs[i], oh[i]) for i in range(len(y_true))])\n",
    "    return acc, f1, kappa, cm, js\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f51bac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run all 3 models\n",
    "preds = {}\n",
    "probs = {}\n",
    "results = {}\n",
    "\n",
    "paths = {\n",
    "    \"indobert\": \"model/indobert-base-p2/model\",\n",
    "    \"indolem\": \"model/indolem-indobert-base-uncased/model\",\n",
    "    \"mdeberta\": \"model/mdeberta-v3-base/model\"\n",
    "}\n",
    "\n",
    "for name, path in paths.items():\n",
    "    tok, model = load_model(path)\n",
    "    # Predictions form all 3 models on full 1900 unseen text\n",
    "    p, pr = predict_batch(texts, tok, model)\n",
    "    preds[name] = p\n",
    "    probs[name] = pr\n",
    "\n",
    "    #compare against Gemini's labls by fidelity measurement: accyracy, macro-f1, kappa, JS divergence, confussion matrix.\n",
    "    acc, f1, kappa, cm, js = evaluate(name, p, pr, y_true)\n",
    "    results[name] = {\n",
    "        \"accuracy\": acc,\n",
    "        \"macro_f1\": f1, # higher better\n",
    "        \"kappa\": kappa, # higher better\n",
    "        \"js_divergence\": js, # smaller better\n",
    "        \"confusion_matrix\": cm # less dissagreement rate better\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf1ce7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserte predictions & probabilities into main dataframe\n",
    "for name in preds:\n",
    "    df[f\"label_{name}\"] = preds[name]\n",
    "    df[f\"prob_{name}_0\"] = probs[name][:,0]\n",
    "    df[f\"prob_{name}_1\"] = probs[name][:,1]\n",
    "    df[f\"prob_{name}_2\"] = probs[name][:,2]\n",
    "df.to_csv(\"student_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5ca3b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: indobert\n",
      "Accuracy: 0.7987421383647799\n",
      "Macro F1: 0.6227909667332411\n",
      "Kappa: 0.465271224478625\n",
      "JS Divergence: 0.34039023744273833\n",
      "Confusion Matrix:\n",
      " [[1313   17   58]\n",
      " [ 142  122   23]\n",
      " [ 118   26   89]]\n",
      "\n",
      "Model: indolem\n",
      "Accuracy: 0.8060796645702306\n",
      "Macro F1: 0.6467614187551173\n",
      "Kappa: 0.4872990385222917\n",
      "JS Divergence: 0.33426381282769807\n",
      "Confusion Matrix:\n",
      " [[1311   31   46]\n",
      " [ 141  123   23]\n",
      " [ 115   14  104]]\n",
      "\n",
      "Model: mdeberta\n",
      "Accuracy: 0.8139412997903563\n",
      "Macro F1: 0.6553380096449603\n",
      "Kappa: 0.512031709942208\n",
      "JS Divergence: 0.33380855455203406\n",
      "Confusion Matrix:\n",
      " [[1317   32   39]\n",
      " [ 123  142   22]\n",
      " [ 116   23   94]]\n"
     ]
    }
   ],
   "source": [
    "# print results\n",
    "for name, r in results.items():\n",
    "    print(\"\\nModel:\", name)\n",
    "    print(\"Accuracy:\", r[\"accuracy\"])\n",
    "    print(\"Macro F1:\", r[\"macro_f1\"])\n",
    "    print(\"Kappa:\", r[\"kappa\"])\n",
    "    print(\"JS Divergence:\", r[\"js_divergence\"])\n",
    "    print(\"Confusion Matrix:\\n\", r[\"confusion_matrix\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6482d2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>kappa</th>\n",
       "      <th>js_divergence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mdeberta</td>\n",
       "      <td>0.813941</td>\n",
       "      <td>0.655338</td>\n",
       "      <td>0.512032</td>\n",
       "      <td>0.333809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>indolem</td>\n",
       "      <td>0.806080</td>\n",
       "      <td>0.646761</td>\n",
       "      <td>0.487299</td>\n",
       "      <td>0.334264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>indobert</td>\n",
       "      <td>0.798742</td>\n",
       "      <td>0.622791</td>\n",
       "      <td>0.465271</td>\n",
       "      <td>0.340390</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  accuracy  macro_f1     kappa  js_divergence\n",
       "2  mdeberta  0.813941  0.655338  0.512032       0.333809\n",
       "1   indolem  0.806080  0.646761  0.487299       0.334264\n",
       "0  indobert  0.798742  0.622791  0.465271       0.340390"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rank model \n",
    "summary = pd.DataFrame([\n",
    "    {\n",
    "        \"model\": name,\n",
    "        \"accuracy\": r[\"accuracy\"],\n",
    "        \"macro_f1\": r[\"macro_f1\"],\n",
    "        \"kappa\": r[\"kappa\"],\n",
    "        \"js_divergence\": r[\"js_divergence\"]\n",
    "    }\n",
    "    for name, r in results.items()\n",
    "])\n",
    "\n",
    "summary.sort_values(by=[\"macro_f1\", \"kappa\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aac561d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "Neutral      1388\n",
       "Inflation     287\n",
       "Deflation     233\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gemini['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4116de0d",
   "metadata": {},
   "source": [
    "# Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310892a0",
   "metadata": {},
   "source": [
    "All 3 models have the samep common thing that is **consistency**. All three models converge on similar patterns: strong Neutral classification, decent Inflation, fragile Deflation. This is exactly how economic news behaves in the wild. Most news is neutral, inflation is frequent, deflation is sparse and ambiguous. The model is not confused in a chaotic way; it’s confused in the same way humans and LLMs are confused. That’s a reassuring baseline.\n",
    "\n",
    "\n",
    "Next, **mDeBERTa being best is not surprising**. The architecture is newer, deeper, and was pretrained with stronger masking objectives. mDeBERTa leads across every fidelity metric, but the margin is not gigantic. This is good news because it signals two things at once. The stronger model generalizes better and copies Gemini’s behaviour more closely. The weaker models are still usable and not collapsing, which means the training pipeline and the dataset are good.\n",
    "\n",
    "\n",
    "The class distribution in the new Gemini-labeled dataset shows a heavy skew toward Neutral. **This is normal for economic text classification**. Most news describes conditions rather than strong directional signals. That skew also explains why accuracy stays high for all models. The macro-F1 tells the real story, and the models sit in the low-to-mid 0.6 range. This is a reasonable level when the teacher model (Gemini) is itself imperfect, and when the minority classes are subtle even for humans.\n",
    "\n",
    "\n",
    "I would say the student models capture the teacher’s logic at a **moderate level**. The agreement is stable and consistent across metrics, so there are no red flags. mDeBERTa is clearly the best reproduction. The kappa around 0.51 tells us that the student is meaningfully aligned with Gemini beyond chance but not a clone. **That is expected because a much smaller model cannot fully match a large proprietary LLM’s contextual understanding.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767dc5cc",
   "metadata": {},
   "source": [
    "# Improvement?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6156047",
   "metadata": {},
   "source": [
    "The current models reproduce Gemini with moderate strength by our limited time and resourcess, but there is still room to tighten the alignment on the minority classes. The Neutral class dominates the dataset, so even small improvements in distinguishing Inflation from Deflation would make the index more sensitive.\n",
    "\n",
    "\n",
    "On a data sclae, our models learned from a single dataset covering a specific slice of the Indonesian economic news space. If expanded with more diverse sources or longer historical periods, the student models gain a richer sense of context. This should improves performance on the lower-frequency classes like Deflation.\n",
    "\n",
    "\n",
    "If having more resources, Longer-context or instruction-tuned models could understand causal chains inside economic narratives better than plain BERT derivatives."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stable_diff_model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
